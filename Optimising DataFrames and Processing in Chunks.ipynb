{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing DataFrames and Processing in Chunks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, we will focus on working with chunked dataframes in Pandas while also optimising the dataframe's memory usage.\n",
    "\n",
    "The dataset we'll be working with contains financial lending data from [Lending Club](https://www.lendingclub.com/), a marketplace for personal loans that matches borrowers with investors.\n",
    "The Lending Club's website lists aprroved loans. Qualified investors can view the borrower's credit score, the purpose of the loan, and other application details. Once the lender is ready to back a loan, it selects the amount off money they want to fund. When the loan amount the borrower requested is fully funded, the borrower receives the money, minus the [origination fee](https://help.lendingclub.com/hc/en-us/articles/214463677) that Lending Club charges.\n",
    "\n",
    "We'll be working with a dataset of loans approved from 2007-2011. In total, the entire dataset consumes about 67 megabytes of memory. For the purposes of our project, we will imagine that we only have 10 megabytes of memory available on our machine.\n",
    "\n",
    "Let's explore the dataset and look for any data quality issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
      "0  1077501  1296599.0     5000.0       5000.0           4975.0   36 months   \n",
      "1  1077430  1314167.0     2500.0       2500.0           2500.0   60 months   \n",
      "2  1077175  1313524.0     2400.0       2400.0           2400.0   36 months   \n",
      "3  1076863  1277178.0    10000.0      10000.0          10000.0   36 months   \n",
      "4  1075358  1311748.0     3000.0       3000.0           3000.0   60 months   \n",
      "\n",
      "  int_rate  installment grade sub_grade                 emp_title emp_length  \\\n",
      "0   10.65%       162.87     B        B2                       NaN  10+ years   \n",
      "1   15.27%        59.83     C        C4                     Ryder   < 1 year   \n",
      "2   15.96%        84.33     C        C5                       NaN  10+ years   \n",
      "3   13.49%       339.31     C        C1       AIR RESOURCES BOARD  10+ years   \n",
      "4   12.69%        67.79     B        B5  University Medical Group     1 year   \n",
      "\n",
      "  home_ownership  annual_inc verification_status   issue_d  loan_status  \\\n",
      "0           RENT     24000.0            Verified  Dec-2011   Fully Paid   \n",
      "1           RENT     30000.0     Source Verified  Dec-2011  Charged Off   \n",
      "2           RENT     12252.0        Not Verified  Dec-2011   Fully Paid   \n",
      "3           RENT     49200.0     Source Verified  Dec-2011   Fully Paid   \n",
      "4           RENT     80000.0     Source Verified  Dec-2011      Current   \n",
      "\n",
      "  pymnt_plan         purpose                 title zip_code addr_state    dti  \\\n",
      "0          n     credit_card              Computer    860xx         AZ  27.65   \n",
      "1          n             car                  bike    309xx         GA   1.00   \n",
      "2          n  small_business  real estate business    606xx         IL   8.72   \n",
      "3          n           other              personel    917xx         CA  20.00   \n",
      "4          n           other              Personal    972xx         OR  17.94   \n",
      "\n",
      "   delinq_2yrs earliest_cr_line  inq_last_6mths  open_acc  pub_rec  revol_bal  \\\n",
      "0          0.0         Jan-1985             1.0       3.0      0.0    13648.0   \n",
      "1          0.0         Apr-1999             5.0       3.0      0.0     1687.0   \n",
      "2          0.0         Nov-2001             2.0       2.0      0.0     2956.0   \n",
      "3          0.0         Feb-1996             1.0      10.0      0.0     5598.0   \n",
      "4          0.0         Jan-1996             0.0      15.0      0.0    27783.0   \n",
      "\n",
      "  revol_util  total_acc initial_list_status  out_prncp  out_prncp_inv  \\\n",
      "0      83.7%        9.0                   f       0.00           0.00   \n",
      "1       9.4%        4.0                   f       0.00           0.00   \n",
      "2      98.5%       10.0                   f       0.00           0.00   \n",
      "3        21%       37.0                   f       0.00           0.00   \n",
      "4      53.9%       38.0                   f     461.73         461.73   \n",
      "\n",
      "    total_pymnt  total_pymnt_inv  total_rec_prncp  total_rec_int  \\\n",
      "0   5863.155187          5833.84          5000.00         863.16   \n",
      "1   1008.710000          1008.71           456.46         435.17   \n",
      "2   3005.666844          3005.67          2400.00         605.67   \n",
      "3  12231.890000         12231.89         10000.00        2214.92   \n",
      "4   3581.120000          3581.12          2538.27        1042.85   \n",
      "\n",
      "   total_rec_late_fee  recoveries  collection_recovery_fee last_pymnt_d  \\\n",
      "0                0.00        0.00                     0.00     Jan-2015   \n",
      "1                0.00      117.08                     1.11     Apr-2013   \n",
      "2                0.00        0.00                     0.00     Jun-2014   \n",
      "3               16.97        0.00                     0.00     Jan-2015   \n",
      "4                0.00        0.00                     0.00     Jun-2016   \n",
      "\n",
      "   last_pymnt_amnt last_credit_pull_d  collections_12_mths_ex_med  \\\n",
      "0           171.62           Jun-2016                         0.0   \n",
      "1           119.66           Sep-2013                         0.0   \n",
      "2           649.91           Jun-2016                         0.0   \n",
      "3           357.48           Apr-2016                         0.0   \n",
      "4            67.79           Jun-2016                         0.0   \n",
      "\n",
      "   policy_code application_type  acc_now_delinq  chargeoff_within_12_mths  \\\n",
      "0          1.0       INDIVIDUAL             0.0                       0.0   \n",
      "1          1.0       INDIVIDUAL             0.0                       0.0   \n",
      "2          1.0       INDIVIDUAL             0.0                       0.0   \n",
      "3          1.0       INDIVIDUAL             0.0                       0.0   \n",
      "4          1.0       INDIVIDUAL             0.0                       0.0   \n",
      "\n",
      "   delinq_amnt  pub_rec_bankruptcies  tax_liens  \n",
      "0          0.0                   0.0        0.0  \n",
      "1          0.0                   0.0        0.0  \n",
      "2          0.0                   0.0        0.0  \n",
      "3          0.0                   0.0        0.0  \n",
      "4          0.0                   0.0        0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "first_five_rows = pd.read_csv('loans_2007.csv', nrows=5)\n",
    "print(first_five_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no glaring data quality issues. However, there are a few opportunities to optimize the data.\n",
    "\n",
    "Now let's decide on the optimal chunk size to use when working with our dataset. Ideally, we will aim to adjust the number of rows so that the memory usage is just under 5 megabytes. This is to allow us an overhead in order to perform calculations using the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 52 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   id                          1000 non-null   int64  \n",
      " 1   member_id                   1000 non-null   float64\n",
      " 2   loan_amnt                   1000 non-null   float64\n",
      " 3   funded_amnt                 1000 non-null   float64\n",
      " 4   funded_amnt_inv             1000 non-null   float64\n",
      " 5   term                        1000 non-null   object \n",
      " 6   int_rate                    1000 non-null   object \n",
      " 7   installment                 1000 non-null   float64\n",
      " 8   grade                       1000 non-null   object \n",
      " 9   sub_grade                   1000 non-null   object \n",
      " 10  emp_title                   949 non-null    object \n",
      " 11  emp_length                  983 non-null    object \n",
      " 12  home_ownership              1000 non-null   object \n",
      " 13  annual_inc                  1000 non-null   float64\n",
      " 14  verification_status         1000 non-null   object \n",
      " 15  issue_d                     1000 non-null   object \n",
      " 16  loan_status                 1000 non-null   object \n",
      " 17  pymnt_plan                  1000 non-null   object \n",
      " 18  purpose                     1000 non-null   object \n",
      " 19  title                       1000 non-null   object \n",
      " 20  zip_code                    1000 non-null   object \n",
      " 21  addr_state                  1000 non-null   object \n",
      " 22  dti                         1000 non-null   float64\n",
      " 23  delinq_2yrs                 1000 non-null   float64\n",
      " 24  earliest_cr_line            1000 non-null   object \n",
      " 25  inq_last_6mths              1000 non-null   float64\n",
      " 26  open_acc                    1000 non-null   float64\n",
      " 27  pub_rec                     1000 non-null   float64\n",
      " 28  revol_bal                   1000 non-null   float64\n",
      " 29  revol_util                  1000 non-null   object \n",
      " 30  total_acc                   1000 non-null   float64\n",
      " 31  initial_list_status         1000 non-null   object \n",
      " 32  out_prncp                   1000 non-null   float64\n",
      " 33  out_prncp_inv               1000 non-null   float64\n",
      " 34  total_pymnt                 1000 non-null   float64\n",
      " 35  total_pymnt_inv             1000 non-null   float64\n",
      " 36  total_rec_prncp             1000 non-null   float64\n",
      " 37  total_rec_int               1000 non-null   float64\n",
      " 38  total_rec_late_fee          1000 non-null   float64\n",
      " 39  recoveries                  1000 non-null   float64\n",
      " 40  collection_recovery_fee     1000 non-null   float64\n",
      " 41  last_pymnt_d                999 non-null    object \n",
      " 42  last_pymnt_amnt             1000 non-null   float64\n",
      " 43  last_credit_pull_d          1000 non-null   object \n",
      " 44  collections_12_mths_ex_med  1000 non-null   float64\n",
      " 45  policy_code                 1000 non-null   float64\n",
      " 46  application_type            1000 non-null   object \n",
      " 47  acc_now_delinq              1000 non-null   float64\n",
      " 48  chargeoff_within_12_mths    1000 non-null   float64\n",
      " 49  delinq_amnt                 1000 non-null   float64\n",
      " 50  pub_rec_bankruptcies        1000 non-null   float64\n",
      " 51  tax_liens                   1000 non-null   float64\n",
      "dtypes: float64(30), int64(1), object(21)\n",
      "memory usage: 1.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "first_1000 = pd.read_csv('loans_2007.csv', nrows=1000)\n",
    "print(first_1000.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 1000 rows consume approx. 1.5 MB. Reading in the file is more efficient if we utilize as much memory as we can. Therefore, we will use chunksizes of 3000 rows, which will use approx. 4.5 MB.\n",
    "Let's verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.580394744873047\n",
      "4.576141357421875\n",
      "4.577898979187012\n",
      "4.579251289367676\n",
      "4.575444221496582\n",
      "4.577326774597168\n",
      "4.575918197631836\n",
      "4.578287124633789\n",
      "4.576413154602051\n",
      "4.57646369934082\n",
      "4.589176177978516\n",
      "4.588043212890625\n",
      "4.594850540161133\n",
      "4.828314781188965\n",
      "0.868586540222168\n"
     ]
    }
   ],
   "source": [
    "loan_chunks = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "for chunk in loan_chunks:\n",
    "    print(chunk.memory_usage(deep=True).sum() / 2**20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data in Chunks\n",
    "\n",
    "Let's familiarise ourselves with the columns to see which ones we can optimise, while working with the data using chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30]\n",
      "[21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22]\n"
     ]
    }
   ],
   "source": [
    "# How many columns have a numeric type? How many have a string type?\n",
    "loan_chunks = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "\n",
    "num_numeric = []\n",
    "num_string = []\n",
    "for chunk in loan_chunks:\n",
    "    total_numeric_cols = chunk.select_dtypes(include=[np.number]).shape[1]\n",
    "    num_numeric.append(total_numeric_cols)\n",
    "    total_string_cols = chunk.select_dtypes(include=['object']).shape[1]\n",
    "    num_string.append(total_string_cols)\n",
    "\n",
    "print(num_numeric)\n",
    "print(num_string)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column': 'term', 'unique values': 2, 'percentage unique': 0.004702010109321735, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'int_rate', 'unique values': 394, 'percentage unique': 0.9262959915363819, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'grade', 'unique values': 7, 'percentage unique': 0.01645703538262607, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'sub_grade', 'unique values': 35, 'percentage unique': 0.08228517691313036, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'emp_title', 'unique values': 30658, 'percentage unique': 76.81976496529605, 'convert to category': False}\n",
      "\n",
      "\n",
      "{'column': 'emp_length', 'unique values': 11, 'percentage unique': 0.026555295367308017, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'home_ownership', 'unique values': 5, 'percentage unique': 0.011755025273304338, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'verification_status', 'unique values': 3, 'percentage unique': 0.007053015163982603, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'issue_d', 'unique values': 55, 'percentage unique': 0.12930527800634772, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'loan_status', 'unique values': 9, 'percentage unique': 0.02115904549194781, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'pymnt_plan', 'unique values': 2, 'percentage unique': 0.004702010109321735, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'purpose', 'unique values': 14, 'percentage unique': 0.03291407076525214, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'title', 'unique values': 21264, 'percentage unique': 50.00705517144066, 'convert to category': False}\n",
      "\n",
      "\n",
      "{'column': 'zip_code', 'unique values': 837, 'percentage unique': 1.967791230751146, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'addr_state', 'unique values': 50, 'percentage unique': 0.11755025273304338, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'earliest_cr_line', 'unique values': 530, 'percentage unique': 1.2468827930174564, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'revol_util', 'unique values': 1119, 'percentage unique': 2.6363529273177053, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'initial_list_status', 'unique values': 1, 'percentage unique': 0.0023510050546608676, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'last_pymnt_d', 'unique values': 103, 'percentage unique': 0.24262696692735325, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'last_credit_pull_d', 'unique values': 108, 'percentage unique': 0.25393242576003383, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'application_type', 'unique values': 1, 'percentage unique': 0.0023510050546608676, 'convert to category': True}\n",
      "\n",
      "\n",
      "{'column': 'id', 'unique values': 3538, 'percentage unique': 100.0, 'convert to category': False}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many unique values are there in each string column? How many of the string columns contain values that are less than 50% unique?\n",
    "loan_chunks = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "\n",
    "unique_values = {}\n",
    "for chunk in loan_chunks:\n",
    "    string_cols = chunk.select_dtypes(include='object')\n",
    "    for col in string_cols.columns:\n",
    "        col_unique_values = string_cols[col].value_counts()\n",
    "        if col in unique_values:\n",
    "            unique_values[col].append(col_unique_values)\n",
    "        else:\n",
    "            unique_values[col] = [col_unique_values]\n",
    "\n",
    "combined_unique_values = {}\n",
    "\n",
    "for col in unique_values:\n",
    "    u_concat = pd.concat(unique_values[col])\n",
    "    u_group = u_concat.groupby(u_concat.index).sum()\n",
    "    col_unique_count = len(u_group)\n",
    "    col_total_count = sum(u_group)\n",
    "    unique_percentage = (col_unique_count / col_total_count) * 100\n",
    "    combined_unique_values[col] = {'column': col, 'unique values': col_unique_count, 'percentage unique': unique_percentage, 'convert to category': unique_percentage < 50}\n",
    "    \n",
    "    print(combined_unique_values[col])\n",
    "    print('\\n')\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `emp_title`, `title` and `id` columns each have more than 50% of values that are unique. This means that they are not suitable to be converted into the category datatype.\n",
    "However, coverting the remaining columns into category datatype may help us save on memory space and improve speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member_id                        3\n",
      "total_rec_int                    3\n",
      "total_pymnt_inv                  3\n",
      "total_pymnt                      3\n",
      "revol_bal                        3\n",
      "recoveries                       3\n",
      "policy_code                      3\n",
      "out_prncp_inv                    3\n",
      "out_prncp                        3\n",
      "total_rec_late_fee               3\n",
      "loan_amnt                        3\n",
      "last_pymnt_amnt                  3\n",
      "total_rec_prncp                  3\n",
      "funded_amnt_inv                  3\n",
      "funded_amnt                      3\n",
      "dti                              3\n",
      "collection_recovery_fee          3\n",
      "installment                      3\n",
      "annual_inc                       7\n",
      "inq_last_6mths                  32\n",
      "total_acc                       32\n",
      "delinq_2yrs                     32\n",
      "pub_rec                         32\n",
      "delinq_amnt                     32\n",
      "open_acc                        32\n",
      "acc_now_delinq                  32\n",
      "tax_liens                      108\n",
      "collections_12_mths_ex_med     148\n",
      "chargeoff_within_12_mths       148\n",
      "pub_rec_bankruptcies          1368\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Which float columns have no missing values and could be candidates for conversion into the integer type?\n",
    "\n",
    "loan_chunks = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "\n",
    "\n",
    "missing = []\n",
    "for chunk in loan_chunks:\n",
    "    floats = chunk.select_dtypes(include='float')\n",
    "    missing.append(floats.apply(pd.isnull).sum())\n",
    "    \n",
    "\n",
    "combined_missing = pd.concat(missing)\n",
    "grouped_missing = combined_missing.groupby(combined_missing.index).sum().sort_values()\n",
    "\n",
    "    \n",
    "print(grouped_missing)\n",
    "\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all of the float columns have missing values. This means that we cannot convert them to integer types, as they do not allow missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.24251079559326\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total memory usage across all chunks\n",
    "\n",
    "loan_chunks = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "\n",
    "total_memory = 0\n",
    "\n",
    "for chunk in loan_chunks:\n",
    "    memory = chunk.memory_usage(deep=True).sum()\n",
    "    total_memory += memory\n",
    "\n",
    "total_memory = total_memory / 2**20 # Convert to megabytes\n",
    "\n",
    "print(total_memory)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Optimising String Columns\n",
    "\n",
    "The best way to achieve substantial memory improvements in our dataframe is to convert the string columns into numeric types wherever possible. We can also optimise the string columns by converting all of the ones we identified as having unique values less than 50% to the category type.\n",
    "\n",
    "If we look back up to where we printed the first 5 rows of the dataset, we can see that the columns `int_rate` and `revol_util` could be converted into numerical types if they are cleaned to remove the % signs. Also, we have seen that the `term` column only has two unique values: `36 months` and `60 months`. This means we could convert the column into a numerical type as long as we make it clear that the units are months.\n",
    "\n",
    "Finally, we can convert any columns that contain information relating to dates into the datetime type. This includes the `issue_d`, `earliest_cr_line`, `last_pymnt_d` and `last_credit_pull_d` columns.\n",
    "\n",
    "In order to keep track of the changes we will build upon the code incrementally. This will also ensure the changes are maintained as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                             object\n",
       "member_id                     float64\n",
       "loan_amnt                     float64\n",
       "funded_amnt                   float64\n",
       "funded_amnt_inv               float64\n",
       "term                           object\n",
       "int_rate                       object\n",
       "installment                   float64\n",
       "grade                          object\n",
       "sub_grade                      object\n",
       "emp_title                      object\n",
       "emp_length                     object\n",
       "home_ownership                 object\n",
       "annual_inc                    float64\n",
       "verification_status            object\n",
       "issue_d                        object\n",
       "loan_status                    object\n",
       "pymnt_plan                     object\n",
       "purpose                        object\n",
       "title                          object\n",
       "zip_code                       object\n",
       "addr_state                     object\n",
       "dti                           float64\n",
       "delinq_2yrs                   float64\n",
       "earliest_cr_line               object\n",
       "inq_last_6mths                float64\n",
       "open_acc                      float64\n",
       "pub_rec                       float64\n",
       "revol_bal                     float64\n",
       "revol_util                     object\n",
       "total_acc                     float64\n",
       "initial_list_status            object\n",
       "out_prncp                     float64\n",
       "out_prncp_inv                 float64\n",
       "total_pymnt                   float64\n",
       "total_pymnt_inv               float64\n",
       "total_rec_prncp               float64\n",
       "total_rec_int                 float64\n",
       "total_rec_late_fee            float64\n",
       "recoveries                    float64\n",
       "collection_recovery_fee       float64\n",
       "last_pymnt_d                   object\n",
       "last_pymnt_amnt               float64\n",
       "last_credit_pull_d             object\n",
       "collections_12_mths_ex_med    float64\n",
       "policy_code                   float64\n",
       "application_type               object\n",
       "acc_now_delinq                float64\n",
       "chargeoff_within_12_mths      float64\n",
       "delinq_amnt                   float64\n",
       "pub_rec_bankruptcies          float64\n",
       "tax_liens                     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert date columns to datetime type\n",
    "chunk_itr = pd.read_csv('loans_2007.csv', chunksize=3000, parse_dates=['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d'])\n",
    "\n",
    "chunk.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                    object\n",
       "member_id                            float64\n",
       "loan_amnt                            float64\n",
       "funded_amnt                          float64\n",
       "funded_amnt_inv                      float64\n",
       "term                                 float32\n",
       "int_rate                             float32\n",
       "installment                          float64\n",
       "grade                                 object\n",
       "sub_grade                             object\n",
       "emp_title                             object\n",
       "emp_length                            object\n",
       "home_ownership                        object\n",
       "annual_inc                           float64\n",
       "verification_status                   object\n",
       "issue_d                       datetime64[ns]\n",
       "loan_status                           object\n",
       "pymnt_plan                            object\n",
       "purpose                               object\n",
       "title                                 object\n",
       "zip_code                              object\n",
       "addr_state                            object\n",
       "dti                                  float64\n",
       "delinq_2yrs                          float64\n",
       "earliest_cr_line              datetime64[ns]\n",
       "inq_last_6mths                       float64\n",
       "open_acc                             float64\n",
       "pub_rec                              float64\n",
       "revol_bal                            float64\n",
       "revol_util                           float32\n",
       "total_acc                            float64\n",
       "initial_list_status                   object\n",
       "out_prncp                            float64\n",
       "out_prncp_inv                        float64\n",
       "total_pymnt                          float64\n",
       "total_pymnt_inv                      float64\n",
       "total_rec_prncp                      float64\n",
       "total_rec_int                        float64\n",
       "total_rec_late_fee                   float64\n",
       "recoveries                           float64\n",
       "collection_recovery_fee              float64\n",
       "last_pymnt_d                  datetime64[ns]\n",
       "last_pymnt_amnt                      float64\n",
       "last_credit_pull_d            datetime64[ns]\n",
       "collections_12_mths_ex_med           float64\n",
       "policy_code                          float64\n",
       "application_type                      object\n",
       "acc_now_delinq                       float64\n",
       "chargeoff_within_12_mths             float64\n",
       "delinq_amnt                          float64\n",
       "pub_rec_bankruptcies                 float64\n",
       "tax_liens                            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_itr = pd.read_csv('loans_2007.csv', chunksize=3000, parse_dates=['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d'])\n",
    "\n",
    "# Remove unwanted characters and convert to numeric types\n",
    "for chunk in chunk_itr:\n",
    "    int_cleaned = chunk['int_rate'].str.rstrip('%')\n",
    "    revol_cleaned = chunk['revol_util'].str.rstrip('%')\n",
    "    term_cleaned = chunk['term'].str.rstrip('months')\n",
    "    \n",
    "    chunk['int_rate'] = pd.to_numeric(int_cleaned, downcast='float')\n",
    "    chunk['revol_util'] = pd.to_numeric(revol_cleaned, downcast='float')\n",
    "    chunk['term'] = pd.to_numeric(term_cleaned, downcast='float')\n",
    "\n",
    "chunk.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grade': 'category', 'sub_grade': 'category', 'emp_length': 'category', 'home_ownership': 'category', 'verification_status': 'category', 'loan_status': 'category', 'pymnt_plan': 'category', 'purpose': 'category', 'zip_code': 'category', 'addr_state': 'category', 'initial_list_status': 'category', 'application_type': 'category'}\n"
     ]
    }
   ],
   "source": [
    "chunk_itr = pd.read_csv('loans_2007.csv', chunksize=3000, parse_dates=['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d'])\n",
    "\n",
    "convert_col_dtypes = {}\n",
    "\n",
    "for chunk in chunk_itr:\n",
    "    int_cleaned = chunk['int_rate'].str.rstrip('%')\n",
    "    revol_cleaned = chunk['revol_util'].str.rstrip('%')\n",
    "    term_cleaned = chunk['term'].str.rstrip('months')\n",
    "    \n",
    "    chunk['int_rate'] = pd.to_numeric(int_cleaned, downcast='float')\n",
    "    chunk['revol_util'] = pd.to_numeric(revol_cleaned, downcast='float')\n",
    "    chunk['term'] = pd.to_numeric(term_cleaned, downcast='float')\n",
    "    \n",
    "# Assess remaining string types to decide whether they should be converted to category types. Compile a dictionary of columns to convert as we read in the csv in the next iteration.\n",
    "    \n",
    "    strings = chunk.select_dtypes(include='object')\n",
    "    for col in strings.columns:\n",
    "        if combined_unique_values[col]['convert to category'] and col not in convert_col_dtypes: # lookup column in analysis carried out above to assess whether it can be converted to category\n",
    "            convert_col_dtypes[col] = 'category'\n",
    "\n",
    "print(convert_col_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.393789291381836\n"
     ]
    }
   ],
   "source": [
    "chunk_itr = pd.read_csv('loans_2007.csv', chunksize=3000, dtype=convert_col_dtypes, parse_dates=['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d'])\n",
    "\n",
    "total_memory = 0\n",
    "\n",
    "for chunk in chunk_itr:\n",
    "    int_cleaned = chunk['int_rate'].str.rstrip('%')\n",
    "    revol_cleaned = chunk['revol_util'].str.rstrip('%')\n",
    "    term_cleaned = chunk['term'].str.rstrip('months')\n",
    "    \n",
    "    chunk['int_rate'] = pd.to_numeric(int_cleaned, downcast='float')\n",
    "    chunk['revol_util'] = pd.to_numeric(revol_cleaned, downcast='float')\n",
    "    chunk['term'] = pd.to_numeric(term_cleaned, downcast='float')\n",
    "    \n",
    "# Calculate total memory footprint now that the string columns have been converted\n",
    "    memory = chunk.memory_usage(deep=True).sum()\n",
    "    total_memory += memory\n",
    "\n",
    "total_memory = total_memory / 2**20\n",
    "print(total_memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by converting the string columns into more appropriate datatypes, we have reduced the memory footprint of the dataset from 65 MB to 19 MB. That's less than a third!\n",
    "\n",
    "## Optimising Numeric Columns\n",
    "\n",
    "Now let's take a look at the numeric columns and see if we can convert any to more appropriate datatypes that will further reduce our memory footprint and runtime. \n",
    "We can convert the float columns that contain missing values into more space efficient subtypes. \n",
    "\n",
    "We could also convert any float columns that don't contain missing values and also represent whole numbers into the integer type. **However**, in our previous analysis above, we found that every float column had missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.525701522827148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                                    object\n",
       "member_id                            float32\n",
       "loan_amnt                            float32\n",
       "funded_amnt                          float32\n",
       "funded_amnt_inv                      float32\n",
       "term                                 float32\n",
       "int_rate                             float32\n",
       "installment                          float32\n",
       "grade                               category\n",
       "sub_grade                           category\n",
       "emp_title                             object\n",
       "emp_length                          category\n",
       "home_ownership                      category\n",
       "annual_inc                           float32\n",
       "verification_status                 category\n",
       "issue_d                       datetime64[ns]\n",
       "loan_status                         category\n",
       "pymnt_plan                          category\n",
       "purpose                             category\n",
       "title                                 object\n",
       "zip_code                            category\n",
       "addr_state                          category\n",
       "dti                                  float32\n",
       "delinq_2yrs                          float32\n",
       "earliest_cr_line              datetime64[ns]\n",
       "inq_last_6mths                       float32\n",
       "open_acc                             float32\n",
       "pub_rec                              float32\n",
       "revol_bal                            float32\n",
       "revol_util                           float32\n",
       "total_acc                            float32\n",
       "initial_list_status                 category\n",
       "out_prncp                            float32\n",
       "out_prncp_inv                        float32\n",
       "total_pymnt                          float32\n",
       "total_pymnt_inv                      float32\n",
       "total_rec_prncp                      float32\n",
       "total_rec_int                        float32\n",
       "total_rec_late_fee                   float32\n",
       "recoveries                           float32\n",
       "collection_recovery_fee              float32\n",
       "last_pymnt_d                  datetime64[ns]\n",
       "last_pymnt_amnt                      float32\n",
       "last_credit_pull_d            datetime64[ns]\n",
       "collections_12_mths_ex_med           float32\n",
       "policy_code                          float32\n",
       "application_type                    category\n",
       "acc_now_delinq                       float32\n",
       "chargeoff_within_12_mths             float32\n",
       "delinq_amnt                          float32\n",
       "pub_rec_bankruptcies                 float32\n",
       "tax_liens                            float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_itr = pd.read_csv('loans_2007.csv', chunksize=3000, dtype=convert_col_dtypes, parse_dates=['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d'])\n",
    "\n",
    "total_memory = 0\n",
    "\n",
    "for chunk in chunk_itr:\n",
    "    int_cleaned = chunk['int_rate'].str.rstrip('%')\n",
    "    revol_cleaned = chunk['revol_util'].str.rstrip('%')\n",
    "    term_cleaned = chunk['term'].str.rstrip('months')\n",
    "    \n",
    "    chunk['int_rate'] = pd.to_numeric(int_cleaned, downcast='float')\n",
    "    chunk['revol_util'] = pd.to_numeric(revol_cleaned, downcast='float')\n",
    "    chunk['term'] = pd.to_numeric(term_cleaned, downcast='float')\n",
    "    \n",
    "    # Convert all float columns into the most efficient subtype\n",
    "    \n",
    "    float_columns = chunk.select_dtypes(include='float')\n",
    "    for col in float_columns.columns:\n",
    "        chunk[col] = pd.to_numeric(float_columns[col], downcast='float')\n",
    "    \n",
    "    # Calculate total memory now numerical types have been optimised\n",
    "    memory = chunk.memory_usage(deep=True).sum()\n",
    "    total_memory += memory\n",
    "\n",
    "total_memory = total_memory / 2**20\n",
    "print(total_memory)\n",
    "\n",
    "\n",
    "chunk.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all of the float columns have been converted into the float32 type. As this datatype uses half as many bits, the memory footprint is reduced substantially.\n",
    "\n",
    "Converting the float types has further reduced the mb used by the dataframe to 15 MB."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
